import os
from dotenv import load_dotenv
import autogen
from memgpt.autogen.memgpt_agent import create_memgpt_autogen_agent_from_config
from memgpt.presets.presets import DEFAULT_PRESET
from memgpt.constants import LLM_MAX_TOKENS

from system_messages import assistant_system_message, planner_system_message, assistant_system_message_short
from splunk_functions import get_fields, splunk_query, functions

load_dotenv(".env")


USE_OPENAI = True
# USE_OPENAI = FalseWhat i
if USE_OPENAI:
    # For demo purposes let's use gpt-4
    model = "gpt-4"

    # This config is for AutoGen agents that are not powered by MemGPT
    config_list = [
        {
            "model": model,
            "api_key": os.getenv("OPENAI_API_KEY"),
        }
    ]

    # This config is for AutoGen agents that powered by MemGPT
    config_list_memgpt = [
        {
            "model": model,
            "preset": DEFAULT_PRESET,
            "model_wrapper": None,
            "model_endpoint_type": "openai",
            "model_endpoint": "https://api.openai.com/v1",
            "context_window": LLM_MAX_TOKENS[model],
        },
    ]

else:
    # Example using LM Studio on a local machine
    # You will have to change the parameters based on your setup

    # Non-MemGPT agents will still use local LLMs, but they will use the ChatCompletions endpoint
    config_list = [
        {
            "model": "NULL",  # not needed
            "api_base": "http://localhost:1234/v1",  # ex. "http://127.0.0.1:5001/v1" if you are using webui, "http://localhost:1234/v1/" if you are using LM Studio
            "api_key": "NULL",  #  not needed
            "api_type": "open_ai",
        },
    ]

    # MemGPT-powered agents will also use local LLMs, but they need additional setup (also they use the Completions endpoint)
    config_list_memgpt = [
        {
            "preset": DEFAULT_PRESET,
            "model": None,  # only required for Ollama, see: https://memgpt.readthedocs.io/en/latest/ollama/
            "model_wrapper": "airoboros-l2-70b-2.1",  # airoboros is the default wrapper and should work for most models
            "model_endpoint_type": "lmstudio",  # can use webui, ollama, llamacpp, etc.
            "model_endpoint": "http://localhost:1234",  # the IP address of your LLM backend
            "context_window": 16385,  # the context window of your model (for Mistral 7B-based models, it's likely 8192)
        },
    ]

# Set to True if you want to print MemGPT's inner workings.
DEBUG = True

interface_kwargs = {
    "debug": DEBUG,
    "show_inner_thoughts": DEBUG,
    "show_function_outputs": DEBUG,
}

llm_config = {"config_list": config_list, "seed": 42, "functions": functions}
llm_config_memgpt = {"config_list": config_list_memgpt, "seed": 42}

# create a UserProxyAgent instance named "user_proxy"
user_proxy = autogen.UserProxyAgent(
    name="user_proxy",
    human_input_mode="TERMINATE",
    max_consecutive_auto_reply=15,
    is_termination_msg=lambda x: x.get("content", "") and x.get("content", "").rstrip().endswith("TERMINATE"),
    code_execution_config={
        "work_dir": "coding",
        "use_docker": False,  # set to True or image name like "python:3" to use docker
    },
    # llm_config=llm_config_turbo,
    system_message="A proxy capable of executing function calls only."
)

user_proxy.register_function(
    function_map={
        'splunk_query': splunk_query,
        "get_fields": get_fields
    }
)


# # The agent playing the role of the product manager (PM)
# pm = autogen.AssistantAgent(
#     name="Product_manager",
#     system_message="A planning agent",
#     llm_config=llm_config,
#     default_auto_reply="...",  # Set a default auto-reply message here (non-empty auto-reply is required for LM Studio)
# )



# create an AssistantAgent named "splunker"
splunker = autogen.AssistantAgent(
    name="Splunk_analyst",
    system_message = assistant_system_message_short,
    llm_config=llm_config
)


# In our example, we swap this AutoGen agent with a MemGPT agent
# This MemGPT agent will have all the benefits of MemGPT, ie persistent memory, etc.
mem_planner = create_memgpt_autogen_agent_from_config(
    name="MemGPT_planner",
    llm_config=llm_config_memgpt,
    system_message=planner_system_message,
    interface_kwargs=interface_kwargs,
    default_auto_reply="...",  # Set a default auto-reply message here (non-empty auto-reply is required for LM Studio)
)

# Initialize the group chat between the user and two LLM agents (PM and coder)
groupchat = autogen.GroupChat(agents=[user_proxy, mem_planner, splunker], messages=[], max_round=12)
manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)


prompt = """
Amber Turing was hoping for Frothly (her beer company) to be acquired by a potential competitor which fell through, but visited their website to find contact information for their executive team. What is the website domain that she visited? Answer example: google.com 

Hints: Search for
index=botsv2 earliest=0 amber
and examine the client_ip field to find Amber's IP address.
Use a query like this to see her Web traffic, using her correct IP address:
index=botsv2 earliest=0 src_ip=1.1.1.1 
Restrict this query to the stream:http sourcetype. There are 198 events.
Look at the site values and look for names of rival beer makers.
"""


# Begin the group chat with a message from the user
user_proxy.initiate_chat(
    manager,
    message=prompt,
)